"""
Llama4æ¨¡å‹ä¼˜åŒ–è„šæœ¬ - å‰ªæä¸MOEè½¬æ¢ (PyTorchç‰ˆæœ¬)
ç‰ˆæœ¬: 2.0
æ›´æ–°:
1. æ·»åŠ GPUè®¾å¤‡æŒ‡å®šåŠŸèƒ½
2. å®ç°å‰ªæå’ŒMOEè½¬æ¢å…·ä½“é€»è¾‘
3. ä¼˜åŒ–é”™è¯¯å¤„ç†
"""

import torch
import torch.nn as nn
from modelscope import Llama4ForConditionalGeneration, AutoProcessor
import os
import argparse
import warnings
from typing import Optional, Literal
from transformers import AutoModelForCausalLM, AutoConfig

class ModelOptimizer:
    def __init__(self, model_path: str, device: str = None):
        """
        åˆå§‹åŒ–ä¼˜åŒ–å™¨
        Args:
            model_path: åŸå§‹æ¨¡å‹è·¯å¾„
            device: è¿è¡Œè®¾å¤‡ (cuda/cpu/cuda:0ç­‰)
        """
        # è‡ªåŠ¨æ£€æµ‹è®¾å¤‡
        if device is None:
            self.device = "cuda" if torch.cuda.is_available() else "cpu"
        else:
            self.device = device
            
        # åŠ è½½æ¨¡å‹
        print(f"â³ ä» {model_path} åŠ è½½æ¨¡å‹åˆ° {self.device}...")
        try:
            self.model = Llama4ForConditionalGeneration.from_pretrained(
                model_path,
                attn_implementation="flash_attention_2",
                device_map="auto",
                torch_dtype=torch.bfloat16,
            )
            self.config = self.model.config
            self._verify_model()
            print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
        except Exception as e:
            print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}")
            raise RuntimeError("æ¨¡å‹åŠ è½½å¤±è´¥ï¼Œè¯·æ£€æŸ¥è·¯å¾„å’Œè®¾å¤‡é…ç½®")

        
    def _verify_model(self):
        """éªŒè¯æ¨¡å‹å®Œæ•´æ€§"""
        # æ£€æŸ¥æ˜¯å¦ä¸ºMoEæ¨¡å‹ - ä¿®å¤é…ç½®è®¿é—®æ–¹å¼
        if hasattr(self.config, 'text_config'):
            text_config = self.config.text_config
            if hasattr(text_config, 'num_local_experts'):
                print(f"ğŸ” æ£€æµ‹åˆ°MoEæ¨¡å‹: {text_config.num_local_experts}ä¸ªä¸“å®¶")
            else:
                warnings.warn("âš ï¸ éMOEæ¨¡å‹ï¼ŒMOEè½¬æ¢å¯èƒ½æ— æ•ˆ")
        else:
            warnings.warn("âš ï¸ æ— æ³•æ£€æµ‹æ¨¡å‹é…ç½®ï¼ŒMOEè½¬æ¢å¯èƒ½æ— æ•ˆ")

    def prune(
        self,
        ratio: float = 0.3,
        prune_type: Literal["structured", "unstructured"] = "structured",
        importance_metric: str = "l1"
    ) -> nn.Module:
        """
        æ‰§è¡Œå‰ªææ“ä½œ
        Args:
            ratio: å‰ªææ¯”ä¾‹ (0-1)
            prune_type: å‰ªæç±»å‹ (structured/unstructured)
            importance_metric: é‡è¦æ€§è¯„ä¼°æ ‡å‡† (l1/l2)
        Returns:
            ä¼˜åŒ–åçš„æ¨¡å‹
        """
        if prune_type == "structured":
            return self._structured_prune(ratio, importance_metric)
        else:
            return self._unstructured_prune(ratio)
       
    def _structured_prune(self, ratio: float, metric: str) -> nn.Module:
        """ç»“æ„åŒ–å‰ªæå®ç°"""
        print(f"ğŸ”§ æ‰§è¡Œç»“æ„åŒ–å‰ªæ (ratio={ratio}, metric={metric})")
        
        # éå†æ‰€æœ‰çº¿æ€§å±‚è¿›è¡Œå‰ªæ
        for name, module in self.model.named_modules():
            if isinstance(module, nn.Linear):
                # è®¡ç®—é€šé“é‡è¦æ€§
                if metric == "l1":
                    importance = module.weight.data.abs().sum(dim=1)
                else:  # l2
                    importance = torch.norm(module.weight.data, p=2, dim=1)
                
                # ç¡®å®šä¿ç•™çš„é€šé“æ•°
                num_keep = int(module.out_features * (1 - ratio))
                if num_keep <= 0:
                    continue
                
                # é€‰æ‹©æœ€é‡è¦çš„é€šé“
                _, keep_indices = torch.topk(importance, num_keep, largest=True)
                
                # å‰ªææƒé‡
                pruned_weight = module.weight.data[keep_indices, :]
                module.weight = nn.Parameter(pruned_weight)
                
                # å‰ªæåç½®
                if module.bias is not None:
                    pruned_bias = module.bias.data[keep_indices]
                    module.bias = nn.Parameter(pruned_bias)
                
                # æ›´æ–°è¾“å‡ºç‰¹å¾æ•°
                module.out_features = num_keep
                print(f"âœ‚ï¸ ç»“æ„åŒ–å‰ªæå±‚ {name}: {module.in_features}x{module.out_features}")
        
        return self.model

    def _unstructured_prune(self, ratio: float) -> nn.Module:
        """éç»“æ„åŒ–å‰ªæå®ç°"""
        print(f"âœ‚ï¸ æ‰§è¡Œéç»“æ„åŒ–å‰ªæ (ratio={ratio})")
        
        # æ”¶é›†æ‰€æœ‰æƒé‡
        all_weights = []
        for name, param in self.model.named_parameters():
            if param.dim() > 1:  # åªå¤„ç†æƒé‡çŸ©é˜µ
                all_weights.append(param.data.view(-1))
        
        # è®¡ç®—å…¨å±€é˜ˆå€¼
        all_weights = torch.cat(all_weights)
        threshold = torch.quantile(all_weights.abs(), ratio)
        
        # åº”ç”¨å‰ªæ
        for name, param in self.model.named_parameters():
            if param.dim() > 1:
                mask = param.data.abs() > threshold
                param.data.mul_(mask.float())
                pruned_count = torch.sum(mask == 0).item()
                total_count = param.numel()
                print(f"âœ‚ï¸ éç»“æ„åŒ–å‰ªæå±‚ {name}: å‰ªæç‡ {pruned_count/total_count:.2%}")
        
        return self.model

    def convert_moe(
        self,
        method: Literal["mean_merge", "utilization_merge", "weighted_merge", "remove"] = "mean_merge",
        dense_size: Optional[int] = None
    ) -> nn.Module:
        """
        MOEè½¬æ¢åŠŸèƒ½
        Args:
            method: ä¸“å®¶åˆå¹¶æ–¹æ³•
            dense_size: ç›®æ ‡Denseå±‚å¤§å° (Noneè¡¨ç¤ºä¿æŒåŸä¸“å®¶æ€»å®¹é‡)
        Returns:
            è½¬æ¢åçš„æ¨¡å‹
        """
        print(f"ğŸ”„ æ‰§è¡ŒMOEè½¬æ¢ (method={method}, dense_size={dense_size})")
        
        if method == "remove":
            return self._remove_moe()
        else:
            return self._merge_experts(method, dense_size)
    
    def _merge_experts(self, method: str, dense_size: int) -> nn.Module:
        """ä¸“å®¶åˆå¹¶æ ¸å¿ƒé€»è¾‘"""
        if method == "mean_merge":
            print("ğŸ“Š ä½¿ç”¨å‡å€¼åˆå¹¶ä¸“å®¶...")
            return self._mean_merge_experts(dense_size)
        elif method == "utilization_merge":
            print("ğŸ“ˆ åŸºäºåˆ©ç”¨ç‡åŠ æƒåˆå¹¶...")
            return self._utilization_merge_experts(dense_size)
        elif method == "weighted_merge":
            print("âš–ï¸ åŸºäºè·¯ç”±æƒé‡åˆå¹¶...")
            return self._weighted_merge_experts(dense_size)


    def _mean_merge_experts(self, dense_size: int) -> nn.Module:
        """å‡å€¼åˆå¹¶ä¸“å®¶ (ä½¿ç”¨å®é™…æƒé‡ç»“æ„)"""
        # ä¿®å¤é…ç½®è®¿é—®æ–¹å¼
        num_layers = self.config.text_config.num_hidden_layers
        for i in range(num_layers):
            layer = self.model.language_model.model.layers[i]
            
            # æ£€æŸ¥MOEç»“æ„ (ä¿®æ­£ä¸ºfeed_forward)
            if hasattr(layer, 'feed_forward') and hasattr(layer.feed_forward, 'experts'):
                ff = layer.feed_forward
                num_experts = len(ff.experts)
                
                # å¹³å‡åˆå¹¶æ‰€æœ‰ä¸“å®¶æƒé‡ (ä½¿ç”¨å®é™…æƒé‡åç§°)
                merged_gate_up = torch.mean(torch.stack(
                    [expert.gate_up_proj.weight.data for expert in ff.experts]), dim=0)
                merged_down = torch.mean(torch.stack(
                    [expert.down_proj.weight.data for expert in ff.experts]), dim=0)
                
                # åˆ›å»ºæ–°çš„ç¨ å¯†å‰é¦ˆå±‚
                new_ffn = nn.Sequential(
                    nn.Linear(merged_gate_up.shape[1], merged_gate_up.shape[0]),
                    nn.GELU(),
                    nn.Linear(merged_down.shape[1], merged_down.shape[0])
                ).to(self.device)
                
                # è®¾ç½®æƒé‡
                new_ffn[0].weight.data = merged_gate_up
                new_ffn[2].weight.data = merged_down
                
                # æ›¿æ¢åŸå§‹MOEæ¨¡å—
                setattr(layer, 'feed_forward', new_ffn)
                print(f"ğŸ”„ åˆå¹¶{num_experts}ä¸ªä¸“å®¶ (layer.{i})")
        
        return self.model
    
    def _utilization_merge_experts(self, dense_size: int) -> nn.Module:
        """åŸºäºåˆ©ç”¨ç‡åŠ æƒåˆå¹¶ä¸“å®¶"""
        print("ğŸ“ˆ åŸºäºä¸“å®¶åˆ©ç”¨ç‡åŠ æƒåˆå¹¶...")
        
        # ä¿®å¤é…ç½®è®¿é—®æ–¹å¼
        num_layers = self.config.text_config.num_hidden_layers
        # éå†æ‰€æœ‰MOEå±‚
        for i in range(num_layers):
            layer = self.model.language_model.model.layers[i]
            
            if hasattr(layer, 'block_sparse_moe'):
                moe = layer.block_sparse_moe
                num_experts = moe.num_experts
                
                # è·å–è·¯ç”±å™¨æƒé‡
                router_weights = moe.gate.weight.data
                
                # è®¡ç®—æ¯ä¸ªä¸“å®¶çš„åˆ©ç”¨ç‡ï¼ˆè·¯ç”±å™¨æƒé‡ç»å¯¹å€¼ä¹‹å’Œï¼‰
                expert_utilization = torch.sum(torch.abs(router_weights), dim=0)
                
                # å½’ä¸€åŒ–å¾—åˆ°æƒé‡å› å­
                weights = expert_utilization / torch.sum(expert_utilization)
                
                # åŠ æƒåˆå¹¶ä¸“å®¶æƒé‡
                merged_w1 = torch.zeros_like(moe.experts[0].w1.weight.data)
                merged_w2 = torch.zeros_like(moe.experts[0].w2.weight.data)
                merged_w3 = torch.zeros_like(moe.experts[0].w3.weight.data)
                
                for idx, expert in enumerate(moe.experts):
                    merged_w1 += weights[idx] * expert.w1.weight.data
                    merged_w2 += weights[idx] * expert.w2.weight.data
                    merged_w3 += weights[idx] * expert.w3.weight.data
                
                # åˆ›å»ºæ–°çš„ç¨ å¯†å‰é¦ˆå±‚
                new_ffn = nn.Sequential(
                    nn.Linear(merged_w1.shape[1], merged_w1.shape[0]),
                    nn.GELU(),
                    nn.Linear(merged_w3.shape[1], merged_w3.shape[0])
                ).to(self.device)
                
                # è®¾ç½®æƒé‡
                new_ffn[0].weight.data = merged_w1
                new_ffn[2].weight.data = merged_w2
                
                # æ›¿æ¢åŸå§‹MOEæ¨¡å—
                setattr(layer, 'feed_forward', new_ffn)
                delattr(layer, 'block_sparse_moe')
                print(f"ğŸ“Š åŸºäºåˆ©ç”¨ç‡åˆå¹¶{num_experts}ä¸ªä¸“å®¶ (layer.{i})")
        
        return self.model
    
    def _weighted_merge_experts(self, dense_size: int) -> nn.Module:
        """åŸºäºè·¯ç”±æƒé‡åˆå¹¶ä¸“å®¶"""
        print("âš–ï¸ åŸºäºè·¯ç”±æƒé‡åŠ æƒåˆå¹¶...")
        
        # ä¿®å¤é…ç½®è®¿é—®æ–¹å¼
        num_layers = self.config.text_config.num_hidden_layers
        # éå†æ‰€æœ‰MOEå±‚
        for i in range(num_layers):
            layer = self.model.language_model.model.layers[i]
            
            if hasattr(layer, 'block_sparse_moe'):
                moe = layer.block_sparse_moe
                num_experts = moe.num_experts
                
                # è·å–è·¯ç”±å™¨æƒé‡
                router_weights = moe.gate.weight.data
                
                # åŠ æƒåˆå¹¶ä¸“å®¶æƒé‡
                merged_w1 = torch.zeros_like(moe.experts[0].w1.weight.data)
                merged_w2 = torch.zeros_like(moe.experts[0].w2.weight.data)
                merged_w3 = torch.zeros_like(moe.experts[0].w3.weight.data)
                
                for idx, expert in enumerate(moe.experts):
                    # ä½¿ç”¨è·¯ç”±å™¨æƒé‡ä½œä¸ºåŠ æƒå› å­
                    weight_factor = router_weights[:, idx].unsqueeze(1)
                    merged_w1 += torch.mean(weight_factor) * expert.w1.weight.data
                    merged_w2 += torch.mean(weight_factor) * expert.w2.weight.data
                    merged_w3 += torch.mean(weight_factor) * expert.w3.weight.data
                
                # åˆ›å»ºæ–°çš„ç¨ å¯†å‰é¦ˆå±‚
                new_ffn = nn.Sequential(
                    nn.Linear(merged_w1.shape[1], merged_w1.shape[0]),
                    nn.GELU(),
                    nn.Linear(merged_w3.shape[1], merged_w3.shape[0])
                ).to(self.device)
                
                # è®¾ç½®æƒé‡
                new_ffn[0].weight.data = merged_w1
                new_ffn[2].weight.data = merged_w2
                
                # æ›¿æ¢åŸå§‹MOEæ¨¡å—
                setattr(layer, 'feed_forward', new_ffn)
                delattr(layer, 'block_sparse_moe')
                print(f"âš–ï¸ åŸºäºè·¯ç”±æƒé‡åˆå¹¶{num_experts}ä¸ªä¸“å®¶ (layer.{i})")
        
        return self.model

    def _remove_moe(self) -> nn.Module:
        """å®Œå…¨ç§»é™¤MOEç»“æ„ï¼Œä»…ä¿ç•™å…±äº«ä¸“å®¶"""
        print("ğŸš® ç§»é™¤MOEç»“æ„ï¼Œä»…ä¿ç•™å…±äº«ä¸“å®¶...")
        
        class SimpleFFN(nn.Module):
            def __init__(self, shared_expert, device):
                super().__init__()
                self.gate_proj = nn.Linear(
                    shared_expert.gate_proj.weight.shape[1],
                    shared_expert.gate_proj.weight.shape[0],
                    bias=shared_expert.gate_proj.bias is not None
                ).to(device)
                self.up_proj = nn.Linear(
                    shared_expert.up_proj.weight.shape[1],
                    shared_expert.up_proj.weight.shape[0],
                    bias=shared_expert.up_proj.bias is not None
                ).to(device)
                self.down_proj = nn.Linear(
                    shared_expert.down_proj.weight.shape[1],
                    shared_expert.down_proj.weight.shape[0],
                    bias=shared_expert.down_proj.bias is not None
                ).to(device)
                # å¤åˆ¶æƒé‡
                self.gate_proj.weight.data = shared_expert.gate_proj.weight.data.clone()
                self.up_proj.weight.data = shared_expert.up_proj.weight.data.clone()
                self.down_proj.weight.data = shared_expert.down_proj.weight.data.clone()
                # å¤åˆ¶biasï¼ˆå¦‚æœæœ‰ï¼‰
                if shared_expert.gate_proj.bias is not None:
                    self.gate_proj.bias.data = shared_expert.gate_proj.bias.data.clone()
                if shared_expert.up_proj.bias is not None:
                    self.up_proj.bias.data = shared_expert.up_proj.bias.data.clone()
                if shared_expert.down_proj.bias is not None:
                    self.down_proj.bias.data = shared_expert.down_proj.bias.data.clone()
            def forward(self, x):
                return self.down_proj(
                    torch.silu(self.gate_proj(x)) * self.up_proj(x)
                )
        
        # ä¿®å¤é…ç½®è®¿é—®æ–¹å¼
        num_layers = self.config.text_config.num_hidden_layers
        for i in range(num_layers):
            layer = self.model.language_model.model.layers[i]
            
            # æ£€æŸ¥æ˜¯å¦å­˜åœ¨MOEç»“æ„ (ä¿®æ­£ä¸ºfeed_forward)
            if hasattr(layer, 'feed_forward') and hasattr(layer.feed_forward, 'shared_expert'):
                ff = layer.feed_forward
                shared_expert = ff.shared_expert
                
                # æ‰“å°shared_expertæƒé‡å’Œbiasçš„shape
                print(f"Layer {i} shared_expert.gate_proj.weight.shape: {shared_expert.gate_proj.weight.shape}")
                print(f"Layer {i} shared_expert.up_proj.weight.shape: {shared_expert.up_proj.weight.shape}")
                print(f"Layer {i} shared_expert.down_proj.weight.shape: {shared_expert.down_proj.weight.shape}")
                if shared_expert.gate_proj.bias is not None:
                    print(f"Layer {i} shared_expert.gate_proj.bias.shape: {shared_expert.gate_proj.bias.shape}")
                if shared_expert.up_proj.bias is not None:
                    print(f"Layer {i} shared_expert.up_proj.bias.shape: {shared_expert.up_proj.bias.shape}")
                if shared_expert.down_proj.bias is not None:
                    print(f"Layer {i} shared_expert.down_proj.bias.shape: {shared_expert.down_proj.bias.shape}")
                
                # æ–°å»ºFFN
                new_ffn = SimpleFFN(shared_expert, self.device)
                # æ‰“å°æ–°å»ºFFNå„å±‚çš„shape
                print(f"Layer {i} new_ffn.gate_proj.weight.shape: {new_ffn.gate_proj.weight.shape}")
                print(f"Layer {i} new_ffn.up_proj.weight.shape: {new_ffn.up_proj.weight.shape}")
                print(f"Layer {i} new_ffn.down_proj.weight.shape: {new_ffn.down_proj.weight.shape}")
                if new_ffn.gate_proj.bias is not None:
                    print(f"Layer {i} new_ffn.gate_proj.bias.shape: {new_ffn.gate_proj.bias.shape}")
                if new_ffn.up_proj.bias is not None:
                    print(f"Layer {i} new_ffn.up_proj.bias.shape: {new_ffn.up_proj.bias.shape}")
                if new_ffn.down_proj.bias is not None:
                    print(f"Layer {i} new_ffn.down_proj.bias.shape: {new_ffn.down_proj.bias.shape}")
                # ç§»é™¤è·¯ç”±å™¨æƒé‡ (å¦‚æœå­˜åœ¨) - åœ¨æ›¿æ¢ä¹‹å‰åˆ é™¤
                if hasattr(ff, 'router'):
                    print(f"âœ‚ï¸ ç§»é™¤è·¯ç”±å™¨å±‚ layer.{i}.router")
                
                # æ›¿æ¢åŸå§‹MOEæ¨¡å—
                setattr(layer, 'feed_forward', new_ffn)
                
                print(f"âœ‚ï¸ ç§»é™¤MOEå±‚ layer.{i}ï¼Œæ›¿æ¢ä¸ºç¨ å¯†å‰é¦ˆå±‚")
        
        return self.model

    
    def save(self, output_path: str):
        """ä¿å­˜ä¼˜åŒ–åæ¨¡å‹"""
        self.model.save_pretrained(output_path)
        print(f"ğŸ’¾ æ¨¡å‹å·²ä¿å­˜è‡³ {output_path}")

if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="Llama4æ¨¡å‹ä¼˜åŒ–å·¥å…· - æ”¯æŒå‰ªæå’ŒMOEè½¬æ¢")
    parser.add_argument("--model_path", type=str, required=True, help="åŸå§‹æ¨¡å‹è·¯å¾„")
    parser.add_argument("--output_path", type=str, required=True, help="è¾“å‡ºè·¯å¾„")
    parser.add_argument("--prune_ratio", type=float, default=0.3, help="å‰ªææ¯”ä¾‹ (0-1ï¼Œä»…åœ¨å¯ç”¨å‰ªææ—¶æœ‰æ•ˆ)")
    parser.add_argument("--prune_type", choices=["structured", "unstructured"], default="structured", help="å‰ªæç±»å‹ (ä»…åœ¨å¯ç”¨å‰ªææ—¶æœ‰æ•ˆ)")
    parser.add_argument("--enable_prune", action="store_true", help="å¯ç”¨å‰ªææ“ä½œ (é»˜è®¤ä¸å‰ªæ)")
    parser.add_argument("--moe_method", choices=["mean_merge", "utilization_merge", "weighted_merge", "remove"], default="remove", help="MOEè½¬æ¢æ–¹æ³•")
    parser.add_argument("--dense_size", type=int, default=None, help="ç›®æ ‡Denseå±‚å¤§å° (Noneè¡¨ç¤ºä¿æŒåŸä¸“å®¶æ€»å®¹é‡)")
    
    args = parser.parse_args()
    
    try:
        optimizer = ModelOptimizer(args.model_path)
        
        # æ ¹æ®å‚æ•°å†³å®šæ˜¯å¦æ‰§è¡Œå‰ªæ
        if args.enable_prune:
            print(f"ğŸ”§ æ‰§è¡Œå‰ªææ“ä½œ (ratio={args.prune_ratio}, type={args.prune_type})")
            optimizer.prune(ratio=args.prune_ratio, prune_type=args.prune_type)
        else:
            print("â­ï¸ è·³è¿‡å‰ªææ“ä½œ")
        
        # æ‰§è¡ŒMOEè½¬æ¢
        optimizer.convert_moe(method=args.moe_method, dense_size=args.dense_size)
        optimizer.save(args.output_path)
        print("ğŸ‰ ä¼˜åŒ–å®Œæˆ!")
    except Exception as e:
        print(f"âŒ é”™è¯¯: {str(e)}")
        warnings.warn("ä¼˜åŒ–è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯ï¼Œè¯·æ£€æŸ¥æ¨¡å‹å’Œå‚æ•°")
